---
layout: docs
title: Creating a Guild package from scratch
description: Follow the steps for recreating the MNIST tutorial source package
group: tutorials
status: unreleased
---

NOTE: The contenet below is in rough-cut format. Consider is document
"pseudo code".

## Identify the candidate project

Identify the project. In this case we're creating a source package for
the MNIST tutorial.

https://github.com/tensorflow/models/tree/master/tutorials/image/mnist

## Name the package

We need a name for the project. Refer to [TODO: need a naming
reference] for package naming conventions.

In this case we'll create a package named `mnist-tutorial-src`.

```shell
$ mkdir mnist-tutorial-src
```

## Initialize the package

Change into the package directory and initialize the package:

```shell
$ guild init --package
```

Edit `GuildPkg` and modify the file to look like this:

```ini
#-*-conf-*-

[package]

  name = mnist-tutorial-src
  version = 1
  source = https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist
  packager = packager

[references]

  github = https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/mnist

```

## Initial package build

Build the package:

```shell
$ guild package
```

At this stage, the package operation will clone the soure repository
and build an empty package. The initial clone operation will take
several minutes. Subsequent packaging downloads will be faster.

You should see some TODO messages:

```shell
Downloading https://github.com/tensorflow/tensorflow.git
Cloning into bare repository '.../mnist-tutorial-src/tensorflow'...
Extracting models/master/tutorials/image/mnist
Using packager
Running build
TODO Perform any build steps here
Running package
TODO Copy packages here
Creating mnist-tutorial-src-1.pkg.tar.xz
```

Let's look at what was created. First let's example the `src`
directory:

```shell
$ find src/
src
src/tensorflow
src/tensorflow/examples
src/tensorflow/examples/tutorials
src/tensorflow/examples/tutorials/mnist
src/tensorflow/examples/tutorials/mnist/BUILD
src/tensorflow/examples/tutorials/mnist/input_data.py
src/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py
src/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py
src/tensorflow/examples/tutorials/mnist/fully_connected_feed.py
src/tensorflow/examples/tutorials/mnist/mnist.py
src/tensorflow/examples/tutorials/mnist/__init__.py
src/tensorflow/examples/tutorials/mnist/mnist_softmax.py
src/tensorflow/examples/tutorials/mnist/mnist_deep.py
```

These are the current source files for the MNIST TensorFlow
tutorial. The list you see may be slightly different.

We'll refer to the `src` directory as `$srcdir` below.

Next let's look at the `pkg` directory:

```shell
$ find pkg/
```

The `pkg` directory contains the exact set of files that are included
in the package archive. It's empty because our initial `packager`
script didn't copy any files into it. As we develop our package, we'll
add files to `pkg` to be included in the package archive.

We'll refer to the `pkg` directory as `$pkgdir` below.

Finally, let's look at the archive that was generated:

```shell
$ tar tf mnist-tutorial-src-1.pkg.tar.xz
./
./.GuildPkg
```

Because the `pkg` directory is empty, the archive only contains the
package definition `GuildPkg`.

The archive is automatically generated by Guild using the package
name, version, and `$pkgdir` contents.

## Create a working source copy

When creating a package, we may modify upstream sources to add or
modify script features. To ensure that we retain an original copy of
the source, create a working copy that we can safely modify:

```shell
$ cp -a src/ working/
```

## Study the project to idenfity its models and operations

Our goal is to create a project that can be trained by simply running
Guild's train command:

```shell
$ guild train
```

To get to this point, we first need to identify a few things:

- Model(s) we want to train
- Training operations and their required flags
- Required datasets

Each project that we work with will be slightly different so we have
to first study the code. This may be as simple as following steps in a
README but in many cases we have to study the code ourselves.

Many TensorFlow training scripts follow the convention of using
Python's argparse module to parameterize inputs for each command,
which simplifies our work. This is the case for the MNIST tutorial
script.

After looking over the MNIST scripts, we see that there are four
training scripts:

| {% link https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax.py %}mnist_softmax.py{% endlink %} | linear regression model without summary stats |
| {% link https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py %}mnist_with_summaries.py{% endlink %} | linear regression model with summary stats |
| {% link https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_softmax_xla.py %}mnist_softmax_xla.py{% endlink %} | linear regression model that uses XLA |
| {% link https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py %}mnist_deep.py{% endlink %} | convolutional model |

Since **mnist_with_summaries.py** implements the same model as
**mnist_softmax.py** and also adds summary stats, we can omit
**mnist_softmax.py**. We can therefore target three models for this
project:

- softmax
- softmax-xla
- deep

Next we need to identify a few important flags that we'll set in the
Guild project file later. These are:

- Location of the data used for training and validation
- Location of the files generated by the script
- Value determining when the training stops

Let's study the command line options (referred to as *flags*)
supported by the softmax model. Change to the `mnist` directory under
the newly created working directory and run the softmax script with
the `--help` option:

```shell
$ cd working/tensorflow/examples/tutorials/mnist
$ python mnist_with_summaries.py --help
```

You should see output similar to this:

```
usage: mnist_with_summaries.py [-h] [--fake_data [FAKE_DATA]]
                               [--max_steps MAX_STEPS]
                               [--learning_rate LEARNING_RATE]
                               [--dropout DROPOUT] [--data_dir DATA_DIR]
                               [--log_dir LOG_DIR]

optional arguments:
  -h, --help            show this help message and exit
  --fake_data [FAKE_DATA]
                        If true, uses fake data for unit testing.
  --max_steps MAX_STEPS
                        Number of steps to run trainer.
  --learning_rate LEARNING_RATE
                        Initial learning rate
  --dropout DROPOUT     Keep probability for training dropout.
  --data_dir DATA_DIR   Directory for storing input data
  --log_dir LOG_DIR     Summaries log directory
```

Reading through the list of optional arguments, we can spot the three
arguments we're interested in:

| `data_dir` | location of the training and validation data |
| `log_dir`  | location of the files generated by the script |
| `max_steps` | steps to train before stopping |

Unfortunately, when we study the arguments for the other two scripts
(**mnist_softmax_xla.py** and **mnist_deep.py**) we see that they only
take the `data_dir` argument. Not only do these other two scripts not
log stats, they hard code all of their other parameters. This makes
them much more difficult to reuse. But that's okay! We'll use this as
an opportunity to improve the scripts via patching.

First, let's start our Guild project using **mnist_with_summaries.py**
which is easier to work with without changes.

## Initialize a Guild project

In the mnist working directory, run Guild `init`:

```shell
$ guild init --force
```

You need to specified the `--force` option because the directory
contains files. Don't worry about this - the default project template
only installs a single `Guild` file and will not overwrite existing
files.

The `init` command generated a `Guild` file, which defines the Guild
project. We'll modify the file to train our softmax example.

Before modifying the file, take a moment to read through it. We'll
strip out most of the default content and annotations for our initial
project, but they provide useful context for future changes.

Modify `Guild` to look contain this:

```ini
#-*-conf-*-

###################################################################
# Project
###################################################################

[project]

  name        = MNIST Tutorial
  description = Softmax and convolutional models trained \
                on the MNIST dataset

###################################################################
# Models
###################################################################

[model "softmax"]

  description = Simple linear regression model
  train = mnist_with_summaries

###################################################################
# Flags
###################################################################

[flags]

  data_dir = data
  log_dir = $RUNDIR
  max_steps = 1000

```

This is a bare-bones project that can be used to train the *softmax*
model.

## Initial softmax training

Let's train the *softmax* model by running this command in the project
directory:


```shell
$ guild train
```

The command will initially download the MNIST dataset and then proceed
to train the softmax model. You should see output like this:

```
Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.
Extracting data/train-images-idx3-ubyte.gz
Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.
Extracting data/train-labels-idx1-ubyte.gz
Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.
Extracting data/t10k-images-idx3-ubyte.gz
Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.
Extracting data/t10k-labels-idx1-ubyte.gz
...
Error logging series values: read_only (was RUNDIR deleted?)
Error logging series values: read_only (was RUNDIR deleted?)
Error logging series values: read_only (was RUNDIR deleted?)
Error logging series values: read_only (was RUNDIR deleted?)
Error logging series values: read_only (was RUNDIR deleted?)
Accuracy at step 0: 0.0936
Accuracy at step 10: 0.702
Accuracy at step 20: 0.8247
Accuracy at step 30: 0.8576
...
```

There's a problem! The message **Error logging series values:
read_only (was RUNDIR deleted?)** describes what's happening. Guild is
attempting to log series values but cannot because `RUNDIR` --- the
directory Guild created for the train operation --- was deleted!

Some of Google's scripts delete `log_dir` before performing the
operation. This is an anti-pattern that we need to fix before Guild
can be used with this script.

If the train operation is still running, terminate it by typing
`CTRL-C`.

Let's modify `mnist_with_summaries.py` to fix this problem. Find the
`main` function in that script. It should look like this:

```python
def main(_):
  if tf.gfile.Exists(FLAGS.log_dir):
    tf.gfile.DeleteRecursively(FLAGS.log_dir)
  tf.gfile.MakeDirs(FLAGS.log_dir)
  train()
```

We can easily spot the culprit! Modify the function to look like this:

```python
def main(_):
  tf.gfile.MakeDirs(FLAGS.log_dir)
  train()
```

Run the `train` operation again:

```shell
$ guild train
```

This time it should run to completion without error messages.

## View training results

Let's use Guild View to visualize the trainings. In a separate
console, change to the `mnist` working directory, and run:

```shell
$ guild view
```

Open {% link http://localhost:6333 %}http://localhost:6333{% endlink
%}. You should see the completed *softmax* run for **MNIST
Tutorial**. Guild View captures details associated with the training
operation, including:

- Command status (completed, terminated, error)
- Flags used with the command
- System attributes at the time of the operation
- System and operation stats throughout the operation
- Command output

In addition, Guild indexes any TensorFlow event scalar summaries
logged by the script. You can view a list of series keys, including
the TensorFlow scalars, using Guild's `list-series` command.

In the same console used to run the `train` command, run this:

```
$ guild list-series -L
```

The `-L` option tells Guild to use the latest run. This is the short
form of the `--latest-run` option. You should see a long list of
series keys, including several that start with `tf/`. These are the
TensorFlow scalar values logged by the script. We can use these to
display high level results in Guild View.

## Add fields to the view

Let's display the training and test accuracies for our model. Edit the
`Guild` project file and add the following to the end of the file:

```ini
###################################################################
# Flags
###################################################################

[field "validation-accuracy"]

  @attrs template "validation-accuracy-field"
  source = series/tf/test/accuracy_1

[field "train-accuracy"]

  @attrs template "train-accuracy-field"
  source = series/tf/train/accuracy_1

[field "loss"]

  @attrs template "loss-field"
  source = series/tf/train/cross_entropy_1

[field "steps"]

  @attrs template "steps-field"
  source = series/tf/train/cross_entropy_1

[field "time"]

  @attrs template "time-field"
  source = series/tf/train/cross_entropy_1
```

Save your changes and reload Guild View. You should now see five high
level summaries:

- Validation Accuracy
- Train Accuracy
- Loss
- Steps
- Time

## Add series to the view

Next we'll add series charts to show how our training progressed over
time.

Modify the `Guild` project file by adding the following to the end of
the file:

``` ini
###################################################################
# Series
###################################################################

[series "accuracy"]

  @attrs template "accuracy-series"
  source = series/tf/.+/accuracy_1

[series "loss"]

  @attrs template "loss-series"
  source = series/tf/train/cross_entropy_1

```

Save your changes and refresh Guild View. You should now see two
additional series charts --- one for accuracy and another for loss.

Note that both training and test accuracy are displayed on the
accuracy chart. That's because we used a regular expression pattern
that matches multiple sources --- in this case both the *train* and
*test* accuracies.

## Adding a deep model

As we saw earlier, the MNIST tutorial code provides a convolutional
model for training MNIST data. However, the script **mnist_deep.py**
has number of important features missing:

- It does not log summary stats, so we can't easily measure how it's
  training or otherwise view that information

- It hard codes a number of important values that should be exposed to
  the user as command line arguments

We could modify the script in place in order to generate a patch, but
it requires so many modifications that it's not practical for
patching. We're better off creating a copy and modifying that. We can
then ship that script with our package, rather than apply a patch to
fix these issues.

We can actually reuse many of the functions defined in `mnist_deep.py`
which may simplify script maintenance.

Create a script named `mnist_deep_with_summaries.py` in the project
directory that looks like this:

## TODO

### TODO: reference this file in source somewhere!

### TODO: Add model and flags

### TODO: Add compare fields

### TODO: Mod copy working instructions above a

- Rename src/tensorflow to src/tensorflow.orig
- Copy src/tensorflow.orig to src/tensorflow.new
- Make changes to src/tensorflow.new

### TODO: additional sources

- Guild.in (copied from tensorflow.new
- tensorflow.patch
- mnist_deep_with_summaries.py

### TODO: packager

- patch files in prepare
- delete build
- copy files in package

### TODO: new vs orig  workflow

Update docs to simply copy the top level dir from its original to
NAME.orig. Then modify the original, using NAME.orig to generate a
patch file.

This is because the Guild project might refer to locations in that top
level dir and so will require the original name.

Also note to copy Guild.in to Guild, if there is one.

### TODO: filling in supported hyper params?

This might not apply here as much, because the example is very
simple. But we could show how additional params could be exposed -
simply by defining the flag or by patching + flag.
